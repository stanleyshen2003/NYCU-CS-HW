{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DM HW1 110705013"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zKI3Jh6sFI_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHSKPTgfWk-0",
        "outputId": "1bbe7953-ca71-440d-fe68-d7f034c6fa6c"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('train.csv')\n",
        "data.drop(columns=['Location', 'Date'], inplace=True)\n",
        "columns = data.columns\n",
        "data['ItemName'] = data['ItemName'].str.replace(' ', '')\n",
        "for column in columns[1:]:\n",
        "    data[column].replace(['#', '*', 'x', 'A'], [None, None, None, None], inplace=True)\n",
        "    data[column] = data[column].apply(pd.to_numeric, errors='coerce')\n",
        "item_amount = data.groupby('ItemName').groups.keys().__len__()\n",
        "data = data.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create new data column, row = items, hour & filling in nan values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "row_n, col_n = data.shape\n",
        "item_names = data[0:item_amount, 0].reshape(-1)\n",
        "dnew = {}\n",
        "for name in item_names:\n",
        "    dnew[name] = []\n",
        "\n",
        "# insert into dictionary\n",
        "for i in range(0, row_n, item_amount):\n",
        "    for j in range(0, item_amount):\n",
        "        dnew[item_names[j]].append(data[i + j, 1:])\n",
        "\n",
        "# fill in nan values with the average of the previous and next value\n",
        "for key in dnew.keys():\n",
        "    dnew[key] = np.array(dnew[key]).flatten()\n",
        "    for i in range(1,dnew[key].shape[0]-1):\n",
        "        if np.isnan(dnew[key][i]):\n",
        "            if not np.isnan(dnew[key][i - 1]) and not np.isnan(dnew[key][i + 1]):\n",
        "                dnew[key][i] = (dnew[key][i- 1] + dnew[key][i+ 1]) / 2\n",
        "\n",
        "# count the number of nan values in each column\n",
        "count = np.zeros(item_amount)\n",
        "for i, key in enumerate(dnew.keys()):\n",
        "    for j in range(dnew[key].shape[0]):\n",
        "        if np.isnan(dnew[key][j]):\n",
        "            count[i] += 1\n",
        "print('number of nans after filling:')\n",
        "for i in range(item_amount):\n",
        "    print(f'{item_names[i]}: {count[i]}', end = ', ')\n",
        "\n",
        "data = pd.DataFrame.from_dict(dnew)\n",
        "data = data.apply(pd.to_numeric, errors='coerce')\n",
        "df = data\n",
        "print(\"\")\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate model input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the nn input data form and compute correlation\n",
        "input_hours = 9\n",
        "daily_data = data.to_numpy()\n",
        "draw_data = []\n",
        "for i in range(0, row_n - input_hours):\n",
        "    temp = daily_data[i:i+input_hours,:].transpose().flatten()\n",
        "    temp = temp.tolist()\n",
        "    temp.append(daily_data[i + input_hours, 9])\n",
        "    draw_data.append(temp)\n",
        "labels = [[item+str(i) for i in range(input_hours)] for item in item_names]\n",
        "labels = np.array(labels).flatten().tolist()\n",
        "print(len(draw_data))\n",
        "data = pd.DataFrame(draw_data, columns=labels + ['predict'])\n",
        "print(\"Corr:\")\n",
        "corr = data.corr()\n",
        "show = corr['predict'].to_numpy()\n",
        "\n",
        "for i in range(0, len(show)-1):\n",
        "    print(labels[i] + ' ', end='')\n",
        "    print(show[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualize for anaylsis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Correlation coefficient matrix for hour data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correlation_matrix = df.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Correlation Coefficient matrix for model input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(80, 80))\n",
        "sns.heatmap(corr, annot=True, fmt='.2f')\n",
        "plt.savefig('correlation.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract specific columns for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# threshold = 0.35\n",
        "# element amount = 24*240\n",
        "input_hours = 9\n",
        "item_names = [ 'CO', 'PM10', 'PM2.5'] #'AMB_TEMP', 'CH4',, 'NMHC', 'NO2', 'NOx',, 'THC'\n",
        "filter_item = []\n",
        "for item in item_names:\n",
        "    if item == 'PM2.5' or item == 'PM10':\n",
        "        for i in range(4, input_hours):\n",
        "            filter_item.append(item + str(i))\n",
        "    else:\n",
        "        for i in range(6, input_hours):\n",
        "            filter_item.append(item + str(i))\n",
        "filter_item.append('predict')\n",
        "new_data = data[filter_item]\n",
        "print(new_data.head())\n",
        "new_data = new_data.dropna()\n",
        "dataset = new_data.to_numpy()\n",
        "dataset = dataset.astype(np.float64)\n",
        "row_n, col_n = dataset.shape\n",
        "\n",
        "print(dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "np.random.shuffle(dataset)\n",
        "trainingX = dataset[:int(0.8 * dataset.shape[0]), :-1]\n",
        "trainingY = dataset[:int(0.8 * dataset.shape[0]), -1].reshape(-1, 1)\n",
        "testingX = dataset[int(0.8 * dataset.shape[0]):, :-1]\n",
        "testingY = dataset[int(0.8 * dataset.shape[0]):, -1].reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# do feature scaling with X^2\n",
        "# min max scaling\n",
        "# these does not help\n",
        "trainingX = np.concatenate((trainingX, np.square(trainingX[:,:-3])), axis=1)\n",
        "testingX = np.concatenate((testingX, np.square(testingX[:,:-3])), axis=1)\n",
        "x_max = np.max(trainingX, axis=0)\n",
        "x_min = np.min(trainingX, axis=0)\n",
        "trainingX = (trainingX - x_min) / (x_max - x_min)\n",
        "testingX = (testingX - x_min) / (x_max - x_min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Closed form solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MSE(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "def close_form(X, Y):\n",
        "    X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
        "    return np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), Y)\n",
        "\n",
        "def close_form_L2(X, Y, l):\n",
        "    X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
        "    return np.dot(np.dot(np.linalg.inv(np.dot(X.T, X) + l * np.eye(X.shape[1])), X.T), Y)\n",
        "\n",
        "weights = close_form_L2(trainingX, trainingY, 0.1)\n",
        "predictedY = np.dot(np.hstack((testingX, np.ones((testingX.shape[0], 1)))), weights)\n",
        "print('MSE:', MSE(testingY, predictedY))\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(testingY, label='True', alpha=0.5)\n",
        "plt.plot(predictedY, label='Predicted', alpha=0.5)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = 'closed_form_weights'\n",
        "print(weights.shape)\n",
        "np.savez(file_path, weight=weights[:-1], intercept=np.array(weights[-1]), x_max=x_max, x_min=x_min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Linear():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.W = np.random.randn(input_size, output_size)\n",
        "        self.intercept = np.random.randn(output_size)\n",
        "        self.input = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.input = x\n",
        "        return np.dot(x, self.W) + self.intercept\n",
        "    \n",
        "    def backward(self, gradient, lr):\n",
        "        # L2 regularize\n",
        "        gradient += 0.01 * np.dot(self.W.T, self.W).item()\n",
        "        gradient_out = np.dot(gradient, self.W.T)\n",
        "        self.W -= lr * np.dot(self.input.T, gradient)\n",
        "        self.intercept -= lr * np.sum(gradient, axis=0)\n",
        "        return gradient_out\n",
        "\n",
        "class Adagrad():\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.G = None\n",
        "    def getlr(self, gradient):\n",
        "        if self.G is None:\n",
        "            self.G = 1\n",
        "        self.G += np.mean(gradient**2)/10\n",
        "        return self.lr / np.sqrt(self.G + 1e-7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainingX.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "def train(lr, model):\n",
        "    writer = SummaryWriter(\"lr_\" + str(lr))\n",
        "    np.random.seed(0)\n",
        "    optimizer = Adagrad(lr=lr)\n",
        "    epochs = 2000000\n",
        "    for i in range(epochs):\n",
        "        output = model.forward(trainingX)\n",
        "        loss = (output - trainingY) ** 2\n",
        "        gradient = 2 * (output - trainingY)\n",
        "        #print(gradient.shape)\n",
        "        lr = optimizer.getlr(gradient)\n",
        "        gradient = model.backward(gradient, lr)\n",
        "        if i % 1000 == 0:\n",
        "            writer.add_scalar('Loss', loss.mean(), i)\n",
        "            print('Epoch:', i, 'Loss:', loss.mean())\n",
        "    np.savez('model.npy', weight=model.W, intercept=model.intercept)\n",
        "    return -loss.mean()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Linear(trainingX.shape[1], 1)\n",
        "print(-train(0.7738, model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Draw result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "record = pd.read_csv('result.csv')\n",
        "record = record.to_numpy()\n",
        "data_size = 4103\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "x_size = record.shape[0]\n",
        "# x axis is record[:, 0]\n",
        "# y axis is record[:, 1]\n",
        "plt.xlabel('Data Size')\n",
        "plt.ylabel('MSE')\n",
        "plt.plot(data_size * record[:, 0], record[:, 1], label='True')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv('test.csv')\n",
        "data.drop(columns=['date'], inplace=True)\n",
        "columns = data.columns\n",
        "\n",
        "data['ItemName'] = data['ItemName'].str.replace(' ', '')\n",
        "print(data.head())\n",
        "for column in columns[1:]:\n",
        "    data[column].replace(['#', '*', 'x', 'A'], [None, None, None, None], inplace=True)\n",
        "    data[column] = data[column].apply(pd.to_numeric, errors='coerce')\n",
        "item_amount = data.groupby('ItemName').groups.keys().__len__()\n",
        "data = data.to_numpy()\n",
        "\n",
        "row_n, col_n = data.shape\n",
        "item_names = data[0:item_amount, 0].reshape(-1)\n",
        "dnew = {}\n",
        "mean = {}\n",
        "for name in item_names:\n",
        "    dnew[name] = []\n",
        "    \n",
        "\n",
        "# insert into dictionary\n",
        "for i in range(0, row_n, item_amount):\n",
        "    for j in range(0, item_amount):\n",
        "        dnew[item_names[j]].append(data[i + j, 1:].tolist())\n",
        "\n",
        "\n",
        "for key in dnew.keys():\n",
        "    dnew[key] = np.array(dnew[key]).flatten()\n",
        "    mean[key] = np.nanmean(dnew[key])\n",
        "    for i in range(1,dnew[key].shape[0]-1):\n",
        "        if np.isnan(dnew[key][i]):\n",
        "            if not np.isnan(dnew[key][i - 1]) and not np.isnan(dnew[key][i + 1]):\n",
        "                dnew[key][i] = (dnew[key][i- 1] + dnew[key][i+ 1]) / 2\n",
        "\n",
        "# count the number of nan values in each column\n",
        "count = np.zeros(item_amount)\n",
        "for i, key in enumerate(dnew.keys()):\n",
        "    for j in range(dnew[key].shape[0]):\n",
        "        if np.isnan(dnew[key][j]):\n",
        "            count[i] += 1\n",
        "            dnew[key][j] = mean[key]\n",
        "print('number of nans after filling:')\n",
        "for i in range(item_amount):\n",
        "    print(f'{item_names[i]}: {count[i]}', end = ', ')\n",
        "    \n",
        "data = pd.DataFrame.from_dict(dnew)\n",
        "data = data.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# get the nn input data form and compute correlation\n",
        "input_hours = 9\n",
        "daily_data = data.to_numpy()\n",
        "print(daily_data.shape)\n",
        "draw_data = []\n",
        "for i in range(0, daily_data.shape[0], input_hours):\n",
        "    temp = daily_data[i:i+input_hours,:].transpose().flatten()\n",
        "    temp = temp.tolist()\n",
        "    #temp.append(daily_data[i + input_hours, 9])\n",
        "    draw_data.append(temp)\n",
        "print(len(draw_data))\n",
        "labels = [[item+str(i) for i in range(input_hours)] for item in item_names]\n",
        "labels = np.array(labels).flatten().tolist()\n",
        "data = pd.DataFrame(draw_data, columns=labels)\n",
        "print(data.size)\n",
        "    \n",
        "# threshold = 0.35\n",
        "# element amount = 24*240\n",
        "input_hours = 9\n",
        "item_names = ['CO', 'PM10', 'PM2.5']# ['CO', 'NMHC', 'NO2', 'NOx', 'PM10', 'PM2.5', 'THC']\n",
        "filter_item = []\n",
        "for item in item_names:\n",
        "    if item == 'PM2.5' or item == 'PM10':\n",
        "        for i in range(4, input_hours):\n",
        "            filter_item.append(item + str(i))\n",
        "    else:\n",
        "        for i in range(6, input_hours):\n",
        "            filter_item.append(item + str(i))\n",
        "new_data = data[filter_item]\n",
        "print(new_data.size)\n",
        "new_data = new_data.dropna()\n",
        "print(new_data.head())\n",
        "dataset = new_data.to_numpy()\n",
        "dataset = dataset.astype(np.float64)\n",
        "row_n, col_n = dataset.shape\n",
        "\n",
        "print(dataset.shape)\n",
        "    \n",
        "model = Linear(dataset.shape[1]+3, 1)\n",
        "loaded = np.load('closed_form_weights.npz')\n",
        "model.W = loaded['weight']\n",
        "model.intercept = loaded['intercept']\n",
        "x_max = loaded['x_max']\n",
        "x_min = loaded['x_min']\n",
        "\n",
        "# do feature scaling with X^2\n",
        "# min max scaling\n",
        "# these does not help\n",
        "dataset = np.concatenate((dataset, np.square(dataset[:,:-3])), axis=1)\n",
        "dataset = (dataset - x_min) / (x_max - x_min)\n",
        "\n",
        "# inference\n",
        "predict = model.forward(dataset)\n",
        "predict = predict.flatten()\n",
        "print(predict.shape)\n",
        "\n",
        "predict = predict.tolist()\n",
        "out = [['index_'+str(i), predict[i]] for i in range(len(predict))]\n",
        "df = pd.DataFrame(out, columns=['index', 'answer'])\n",
        "df.to_csv('predict.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
